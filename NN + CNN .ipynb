{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of kaggleComp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSeXdD8BJhXP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZucOaWqCTPX"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomDNADataset(Dataset):\n",
        "    def __init__(self, feature_file,label_file, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(label_file)\n",
        "        self.img_features = pd.read_csv(feature_file)\n",
        "        # pandas read the data \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        # no transform is used here \n",
        "        self.dic= {'-':0,'A' :2**2 , 'C':2**3,'T':2**4,'G':2**5,'N':0}\n",
        "        # I learned the sequence of dana from file:///D:/downloads/dna-and-animal-classification.pdf\n",
        "        #and from http://ircamera.as.arizona.edu/Astr2016/text/nucleicacid1.htm \n",
        "        # which showed that there are 4 dominante letters to determine the sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "        # this is just the number rows or samples in the input data as used by \n",
        "        # Dataset class \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        " ############# DNA transform ######################     \n",
        "        DNA = self.img_features.iloc[idx,1]\n",
        "        # so DNA is getting the img_features panda csv read and we are taking the second\n",
        "        #colmn , the first is just ids . idx is generated by pytorch randomly depending on\n",
        "        # if we shuffle the data or not and the number of batch size (part of Dataset class which\n",
        "        # we inheret from )\n",
        "        n = []\n",
        "        Our_pad = 1296-len(DNA)\n",
        "        # here I am using 1296 which is 36*36 , I choose this number based on \n",
        "        # the maximum individual row I found which was around 1058 ,this is used to \n",
        "        # generate a padding which is some how consistent along all .I also chose \n",
        "        # 36 so that when we do conv and maxpool we get a nice number which is \n",
        "        # dividable by 2 How to fix RuntimeError \"Expected object \n",
        "        #of scalar type Float but got scalar type Double for argument\"\n",
        "        \n",
        "        for i in DNA:# DNA is our row where we itterate \n",
        "          if i in self.dic:\n",
        "            n.append(float(self.dic[i]))\n",
        "          else:\n",
        "            n.append(1)\n",
        "        for pad in range(Our_pad):# we pad the end of the sequence with zeros \n",
        "          n.append(float(0))\n",
        "        l = np.array(n , dtype=np.float32)\n",
        "        # this was very trick where the base type of any np array is float 64 or \n",
        "        # double but the base type for torch is float 32 so if we convert \n",
        "        # a np array to torch we need to first change it to float32 or we get an error \n",
        "        # which says \n",
        "        Data_array = torch.from_numpy(l)\n",
        "        #transfer a np to torch \n",
        "        \n",
        "        \n",
        "\n",
        "        #DNA_dense = Data_array\n",
        "        # depending on the type of network we can keep it flat or \n",
        "        # change the dim (reshape)\n",
        "\n",
        "        DNA_image = Data_array.reshape(1,36,36)\n",
        "        # reshape the dim to be in the form of an image of pix 1 which is \n",
        "        # usally the RGB or gray channel , in our case it is gray ,\n",
        "        # then 36 by 36 which what our padding is doing \n",
        "\n",
        "\n",
        "#################################################\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        # we get the labels from a different file \n",
        "\n",
        "        return DNA_image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgDwl95Qj77D"
      },
      "source": [
        "class NN(nn.Module):\n",
        "  # this is just a feed forward NN \n",
        "  def __init__ (self,input_size , num_classes):\n",
        "    super(NN,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size , 50)\n",
        "    self.fc2 = nn.Linear(50 , 500)\n",
        "    self.fc3 = nn.Linear(500 , 250)\n",
        "    self.fc4 = nn.Linear(250 , 1000)\n",
        "    self.fc5 = nn.Linear(1000,num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.fc3(x))\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.fc4(x))\n",
        "    \n",
        "    x = self.fc5(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNONH3x3Ie1J"
      },
      "source": [
        " # model = NN(36*36,1202)\n",
        " # x = torch.randn(64,36*36)\n",
        " # print(model(x).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bCRNEWgneDO"
      },
      "source": [
        "full_dataset= CustomDNADataset(feature_file='/content/drive/MyDrive/deep learning course/dna-barcode-classification/train_features.csv',label_file='/content/drive/MyDrive/deep learning course/dna-barcode-classification/train_labels.csv')\n",
        "# so pass the whole set by specifing the path of file to be inputed to our custum class \n",
        "\n",
        "Test_dataset = CustomDNADataset(feature_file='/content/drive/MyDrive/deep learning course/dna-barcode-classification/test_features.csv',label_file='/content/drive/MyDrive/deep learning course/dna-barcode-classification/train_labels - Copy.csv')\n",
        "# for the test_dataset I am using fake label data which is not accually there just so that I can use \n",
        "# my custom class , it has the same len as the train data ( I choped some in csv file )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFURohSJ6PrM",
        "outputId": "f89d0bec-0eae-41e8-f43e-ea5b8402b581"
      },
      "source": [
        "len(full_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12906"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCo0WyqQCoFD"
      },
      "source": [
        "train_size = int(0.9 * len(full_dataset))\n",
        "# this is a creative way to create a validation set by spliting the dataset \n",
        "validation_size = len(full_dataset) - train_size\n",
        "train_dataset, validation_dataset = torch.utils.data.random_split(full_dataset, [train_size, validation_size])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xE9jN3oCqLy"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self,in_channels = 1 , num_classes =None):\n",
        "    super(CNN,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 9,(3,3))\n",
        "    self.pool = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))#17 * 17\n",
        "    self.conv2 = nn.Conv2d(9 ,18 , kernel_size=(3,3))\n",
        "    self.fc1 = nn.Linear(18*7*7, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # use print to make sure that the Linear gets the correct num of nodes in \n",
        "    # the creation phase (18*7*7)\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    #print(x.shape)\n",
        "    x = self.pool(x)\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    #print(x.shape)\n",
        "    x = self.pool(x)\n",
        "    #print(x.shape)\n",
        "    x = x.reshape(x.shape[0],-1) # reshape to be fully connected layer \n",
        "    #print(x.shape)\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsng1QMF-WGD",
        "outputId": "0dbc96a5-f16b-4055-d4dd-25aed663e1f9"
      },
      "source": [
        "#model = CNN()\n",
        "#x = torch.randn(120,1,36,36)\n",
        "#print(model(x).shape)\n",
        "data_labels  = pd.read_csv(\"/content/drive/MyDrive/deep learning course/dna-barcode-classification/train_labels.csv\")\n",
        "\n",
        "data_labels['labels'].value_counts()\n",
        "data_labels['labels'].max()\n",
        "# we are using value_counts() to know the number of classes but \n",
        "# it acually didnt work because the number of classes is less than the \n",
        "# max scaller number of the labels which was 1213 and so \n",
        "# this is why I choose 1214 as the number of classes .\n",
        "# even though we will have extra classes but that is ok   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1213"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clnCengJ-Yvk"
      },
      "source": [
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        " \n",
        "\n",
        "\n",
        "# hyper parameters \n",
        "input_size = 36*36\n",
        "in_channel = 1\n",
        "num_classes = 1214\n",
        "learning_rate = 0.001\n",
        "batch_size = 15\n",
        "# the batch size appered to be a significant factor on the effectivness of the training\n",
        "# 15 is found to be a good num \n",
        "num_epochs = 3\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0gw1rULGaiA"
      },
      "source": [
        "#initialize the network \n",
        "model = CNN(1,num_classes).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6yKndOhlb2n"
      },
      "source": [
        "#model = NN(input_size,num_classes).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3woOeWyEeC1"
      },
      "source": [
        "#load data \n",
        "train_loader = DataLoader(full_dataset , batch_size=batch_size , \n",
        "                          shuffle = True)\n",
        "Validation_loader = DataLoader(validation_dataset , batch_size=batch_size , \n",
        "                          shuffle = False)\n",
        "test_loader = DataLoader(Test_dataset  , batch_size=1 , shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anAlU4hIFFx_"
      },
      "source": [
        "#loss and optimizer \n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr= learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAwAUfwKMdY-"
      },
      "source": [
        "\n",
        "for epoch in range (num_epochs):\n",
        "  for batch_ind ,(data, targets) in enumerate(train_loader):\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "\n",
        "    #print(data.shape)\n",
        "    scores = model(data)\n",
        "    print(model(data)[0])\n",
        "    print(targets.shape)\n",
        "    print(criterion(scores , targets))\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1YZCfNiFsza"
      },
      "source": [
        "# train Network\n",
        "for epoch in range (num_epochs):\n",
        "  for batch_ind ,(data, targets) in enumerate(train_loader):\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "    # we are coping our data from the cpu to the GPU \n",
        "\n",
        "    #print(data.shape)\n",
        "\n",
        "    # forward\n",
        "    scores = model(data)\n",
        "    # this were we send our data to with shape ([batchsize , chennel size , 36,36]in to\n",
        "    # 15,1214 out as our predition )\n",
        "    loss = criterion(scores , targets)\n",
        "    # loss which is the mean square error of the scores which is the weights with the\n",
        "    # targets which should be (15) and the yi-y\n",
        "\n",
        "    # backword \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # compute the gradients \n",
        "\n",
        "\n",
        "    # gradient descent or adam step\n",
        "\n",
        "    optimizer.step()\n",
        "    #print(loss.item())\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    #check the accuracy on validation data \n",
        "\n",
        "  def check_accuracy (loader,model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x,y in loader:\n",
        "        x = x.to(device = device )\n",
        "        y = y.to(device = device)\n",
        "        # we are memicing the model and loading the data but this time with a \n",
        "        #trained model \n",
        "        \n",
        "        scores = model(x)\n",
        "        # our (15,1214) tensor \n",
        "        _,prediction = scores.max(1)\n",
        "        # maximum value in the tensor (1) we need the index of it\n",
        "        #print('model prediction = ',prediction , 'y = ',y)\n",
        "        num_correct += (prediction == y).sum()\n",
        "        # compare predition with y our target and get 0 for no and 1 for true or yes \n",
        "        num_samples += prediction.size(0)\n",
        "        # compute the total num of samples \n",
        "\n",
        "      print(f'for{num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "    model.train()\n",
        "\n",
        "  check_accuracy(train_loader,model)\n",
        "  #check_accuracy(Validation_loader , model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv_ZZh3J60WR"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/99_33model.pt')\n",
        "# save the model to the Path which saves the state_dict(from url https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oiLdwchNts_"
      },
      "source": [
        "  def check_accuracy (loader,model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "    your_file = open('newResult.csv', 'ab')\n",
        "    # we create a csv file or open it 'ab' for append binery \n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x,y in loader:\n",
        "        x = x.to(device = device )\n",
        "        y = y.to(device = device)\n",
        "        \n",
        "        scores = model(x)\n",
        "        _,prediction = scores.max(1)\n",
        "        #print('model prediction = ',prediction , 'y = ',y)\n",
        "        num_correct += (prediction == y).sum()\n",
        "        np.savetxt(your_file,prediction.cpu())\n",
        "        num_samples += prediction.size(0)\n",
        "\n",
        "      print(f'for{num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "    your_file.close()\n",
        "    model.train()\n",
        "\n",
        "  check_accuracy(test_loader,model)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEahAqcqrGpJ"
      },
      "source": [
        "def write_csv_results (loader,model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "    your_file = open('resultsWithforeign_75.csv', 'ab')\n",
        "    # we create a csv file or open it 'ab' for append binery \n",
        "    sm = torch.nn.Softmax()\n",
        "    # we use a softmax to compute the probability of each guess of the network \n",
        "    \n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x,_ in loader:\n",
        "        x = x.to(device = device )\n",
        "       \n",
        "        \n",
        "        scores = model(x)\n",
        "        _,prediction = scores.max(1)\n",
        "        #print(scores.max())\n",
        "        #print('model prediction = ',prediction , 'y = ',y)\n",
        "        probabilities = sm(scores) \n",
        "        #print(probabilities.max())\n",
        "        #print(prediction)\n",
        "        if probabilities.max() < 0.75 :\n",
        "          prediction = torch.tensor([-1])\n",
        "          np.savetxt(your_file,prediction.cpu())\n",
        "          # we save the prediction in the open file but first we have to \n",
        "          # send it back to being a cpu \n",
        "        else:\n",
        "          np.savetxt(your_file,prediction.cpu().numpy())  \n",
        "        \n",
        "        \n",
        "\n",
        "      model.train()\n",
        "      your_file.close()\n",
        "\n",
        "write_csv_results(test_loader,model)\n",
        "# using our def \n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}