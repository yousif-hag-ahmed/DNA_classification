{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transferLearning_ kaggleComp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSeXdD8BJhXP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets ,models, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "#  https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/62840b1eece760d5e42593187847261f/transfer_learning_tutorial.ipynb#scrollTo=dF0_RiFvCP4s\n"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZucOaWqCTPX"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomDNADataset(Dataset):\n",
        "    def __init__(self, feature_file,label_file, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(label_file)\n",
        "        self.img_features = pd.read_csv(feature_file)\n",
        "        # pandas read the data \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        # no transform is used here \n",
        "        self.dic= {'-':0,'A' :2**2 , 'C':2**4,'T':2**6,'G':2**7,'N':0}\n",
        "        # I learned the sequence of dana from file:///D:/downloads/dna-and-animal-classification.pdf\n",
        "        #and from http://ircamera.as.arizona.edu/Astr2016/text/nucleicacid1.htm \n",
        "        # which showed that there are 4 dominante letters to determine the sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "        # this is just the number rows or samples in the input data as used by \n",
        "        # Dataset class \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        " ############# DNA transform ######################     \n",
        "        DNA = self.img_features.iloc[idx,1]\n",
        "        # so DNA is getting the img_features panda csv read and we are taking the second\n",
        "        #colmn , the first is just ids . idx is generated by pytorch randomly depending on\n",
        "        # if we shuffle the data or not and the number of batch size (part of Dataset class which\n",
        "        # we inheret from )\n",
        "        n = []\n",
        "        Our_pad = 1296-len(DNA)\n",
        "        # here I am using 1296 which is 36*36 , I choose this number based on \n",
        "        # the maximum individual row I found which was around 1058 ,this is used to \n",
        "        # generate a padding which is some how consistent along all .I also chose \n",
        "        # 36 so that when we do conv and maxpool we get a nice number which is \n",
        "        # dividable by 2 How to fix RuntimeError \"Expected object \n",
        "        #of scalar type Float but got scalar type Double for argument\"\n",
        "        \n",
        "        for i in DNA:# DNA is our row where we itterate \n",
        "          if i in self.dic:\n",
        "            n.append(float(self.dic[i]))\n",
        "          else:\n",
        "            n.append(1)\n",
        "        for pad in range(Our_pad):# we pad the end of the sequence with zeros \n",
        "          n.append(float(0))\n",
        "        l = np.array(n , dtype=np.float32)\n",
        "        # this was very trick where the base type of any np array is float 64 or \n",
        "        # double but the base type for torch is float 32 so if we convert \n",
        "        # a np array to torch we need to first change it to float32 or we get an error \n",
        "        # which says \n",
        "        Data_array = torch.from_numpy(l)\n",
        "        #transfer a np to torch \n",
        "        \n",
        "        \n",
        "\n",
        "        #DNA_dense = Data_array\n",
        "        # depending on the type of network we can keep it flat or \n",
        "        # change the dim (reshape)\n",
        "\n",
        "        DNA_image = Data_array.reshape(1,36,36)\n",
        "        DNA_image = DNA_image.repeat(3,1,1)\n",
        "\n",
        "        # reshape the dim to be in the form of an image of pix 1 which is \n",
        "        # usally the RGB or gray channel , in our case it is gray ,\n",
        "        # then 36 by 36 which what our padding is doing \n",
        "        #for a 3 chennel image insted of 1 \n",
        "\n",
        "\n",
        "#################################################\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        # we get the labels from a different file \n",
        "\n",
        "        return DNA_image, label"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsniYJsGO640",
        "outputId": "998c976a-0a98-43ac-df73-e7bd2b1ab7c6"
      },
      "source": [
        "im = torch.ones(1,36,36)\n",
        "im.shape\n",
        "im = im.repeat(3,1,1)\n",
        "\n",
        "im.shape\n",
        "\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 36, 36])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bCRNEWgneDO"
      },
      "source": [
        "train_features_path = '/content/drive/MyDrive/Colab Notebooks/deep learning CSCI570/deep learning course/dna-barcode-classification/train_features.csv'\n",
        "test_features_path = '/content/drive/MyDrive/Colab Notebooks/deep learning CSCI570/deep learning course/dna-barcode-classification/test_features.csv'\n",
        "train_labels_path = '/content/drive/MyDrive/Colab Notebooks/deep learning CSCI570/deep learning course/dna-barcode-classification/train_labels.csv'\n",
        "test_labels_path = '/content/drive/MyDrive/Colab Notebooks/deep learning CSCI570/deep learning course/dna-barcode-classification/train_labels - Copy.csv'\n",
        "\n",
        "\n",
        "full_dataset= CustomDNADataset(feature_file=train_features_path,label_file=train_labels_path)\n",
        "# so pass the whole set by specifing the path of file to be inputed to our custum class \n",
        "\n",
        "Test_dataset = CustomDNADataset(feature_file=test_features_path,label_file=test_labels_path)\n",
        "# for the test_dataset I am using fake label data which is not accually there just so that I can use \n",
        "# my custom class , it has the same len as the train data ( I choped some in csv file )"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFURohSJ6PrM",
        "outputId": "f59750b9-b107-455a-80f8-93750ccd7568"
      },
      "source": [
        "len(full_dataset)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12906"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCo0WyqQCoFD"
      },
      "source": [
        "train_size = int(0.80 * len(full_dataset))\n",
        "# this is a creative way to create a validation set by spliting the dataset \n",
        "validation_size = len(full_dataset) - train_size\n",
        "train_dataset, validation_dataset = torch.utils.data.random_split(full_dataset, [train_size, validation_size])\n"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr4SeMT6LbPu"
      },
      "source": [
        "dataloaders = {'train': torch.utils.data.DataLoader(train_dataset, batch_size=4,\n",
        "                                             shuffle=True, num_workers=2),\n",
        "               'val' : torch.utils.data.DataLoader(validation_dataset, batch_size=4,\n",
        "                                             shuffle=True, num_workers=2)}\n",
        "dataset_sizes = {'train': len(train_dataset) ,\n",
        "                 'val' : len(validation_dataset)}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clnCengJ-Yvk"
      },
      "source": [
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        " \n",
        "\n",
        "\n",
        "# hyper parameters \n",
        "input_size = 36*36\n",
        "in_channel = 1\n",
        "num_classes = 1214\n",
        "learning_rate = 0.001\n",
        "batch_size = 15\n",
        "# the batch size appered to be a significant factor on the effectivness of the training\n",
        "# 15 is found to be a good num \n",
        "num_epochs = 3\n",
        "#\n"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYSEkhF0N4jT"
      },
      "source": [
        ""
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2uIUrDHCP4v"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYrLz-yLCP4x"
      },
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvM5XDWZCP4x"
      },
      "source": [
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCZXJlBrCP4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c46162-2216-4146-8d1b-c54e531c96ff"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=25)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 6.1650 Acc: 0.0874\n",
            "val Loss: 4.8929 Acc: 0.2010\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 3.7316 Acc: 0.3289\n",
            "val Loss: 2.2707 Acc: 0.5488\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 2.0217 Acc: 0.6185\n",
            "val Loss: 1.0971 Acc: 0.7754\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 1.1175 Acc: 0.7963\n",
            "val Loss: 0.6692 Acc: 0.8613\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 0.6862 Acc: 0.8771\n",
            "val Loss: 0.4904 Acc: 0.8962\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.4520 Acc: 0.9204\n",
            "val Loss: 0.3904 Acc: 0.9171\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.3258 Acc: 0.9421\n",
            "val Loss: 0.3261 Acc: 0.9303\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.1715 Acc: 0.9762\n",
            "val Loss: 0.2587 Acc: 0.9435\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.1263 Acc: 0.9854\n",
            "val Loss: 0.2465 Acc: 0.9438\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.1098 Acc: 0.9876\n",
            "val Loss: 0.2381 Acc: 0.9485\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.0998 Acc: 0.9896\n",
            "val Loss: 0.2413 Acc: 0.9516\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.0958 Acc: 0.9904\n",
            "val Loss: 0.2335 Acc: 0.9520\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.0938 Acc: 0.9908\n",
            "val Loss: 0.2292 Acc: 0.9512\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.0883 Acc: 0.9916\n",
            "val Loss: 0.2277 Acc: 0.9531\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.0866 Acc: 0.9918\n",
            "val Loss: 0.2221 Acc: 0.9547\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 0.0809 Acc: 0.9921\n",
            "val Loss: 0.2221 Acc: 0.9539\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 0.0822 Acc: 0.9913\n",
            "val Loss: 0.2157 Acc: 0.9539\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 0.0835 Acc: 0.9923\n",
            "val Loss: 0.2279 Acc: 0.9508\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 0.0797 Acc: 0.9933\n",
            "val Loss: 0.2243 Acc: 0.9543\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 0.0799 Acc: 0.9930\n",
            "val Loss: 0.2181 Acc: 0.9551\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 0.0793 Acc: 0.9939\n",
            "val Loss: 0.2221 Acc: 0.9531\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 0.0805 Acc: 0.9927\n",
            "val Loss: 0.2233 Acc: 0.9539\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 0.0776 Acc: 0.9932\n",
            "val Loss: 0.2188 Acc: 0.9539\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 0.0795 Acc: 0.9938\n",
            "val Loss: 0.2239 Acc: 0.9543\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 0.0779 Acc: 0.9933\n",
            "val Loss: 0.2161 Acc: 0.9555\n",
            "\n",
            "Training complete in 36m 19s\n",
            "Best val Acc: 0.955461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0gw1rULGaiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6df3d9-7914-45f7-c263-0d0743b5d630"
      },
      "source": [
        "test_loader = DataLoader(Test_dataset  , batch_size=1 , shuffle=False)\n",
        "def write_csv_results (loader,model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "    your_file = open('resultsWithforeign_75_transferLearningCNN.csv', 'ab')\n",
        "    # we create a csv file or open it 'ab' for append binery \n",
        "    sm = torch.nn.Softmax()\n",
        "    # we use a softmax to compute the probability of each guess of the network \n",
        "    \n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x,_ in loader:\n",
        "        x = x.to(device = device )\n",
        "       \n",
        "        \n",
        "        scores = model(x)\n",
        "        _,prediction = scores.max(1)\n",
        "        #print(scores.max())\n",
        "        #print('model prediction = ',prediction , 'y = ',y)\n",
        "        probabilities = sm(scores) \n",
        "        #print(probabilities.max())\n",
        "        #print(prediction)\n",
        "        if probabilities.max() < 0.75 :\n",
        "          prediction = torch.tensor([-1])\n",
        "          np.savetxt(your_file,prediction.cpu())\n",
        "          # we save the prediction in the open file but first we have to \n",
        "          # send it back to being a cpu \n",
        "        else:\n",
        "          np.savetxt(your_file,prediction.cpu().numpy())  \n",
        "        \n",
        "        \n",
        "\n",
        "      model.train()\n",
        "      your_file.close()\n",
        "\n",
        "write_csv_results(test_loader,model_ft)\n",
        "# using our def \n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6yKndOhlb2n"
      },
      "source": [
        "#model = NN(input_size,num_classes).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anAlU4hIFFx_"
      },
      "source": [
        "#loss and optimizer \n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr= learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAwAUfwKMdY-"
      },
      "source": [
        "\n",
        "for epoch in range (num_epochs):\n",
        "  for batch_ind ,(data, targets) in enumerate(train_loader):\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "\n",
        "    #print(data.shape)\n",
        "    scores = model(data)\n",
        "    print(model(data)[0])\n",
        "    print(targets.shape)\n",
        "    print(criterion(scores , targets))\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1YZCfNiFsza"
      },
      "source": [
        "# train Network\n",
        "for epoch in range (num_epochs):\n",
        "  for batch_ind ,(data, targets) in enumerate(train_loader):\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.to(device = device)\n",
        "    # we are coping our data from the cpu to the GPU \n",
        "\n",
        "    #print(data.shape)\n",
        "\n",
        "    # forward\n",
        "    scores = model(data)\n",
        "    # this were we send our data to with shape ([batchsize , chennel size , 36,36]in to\n",
        "    # 15,1214 out as our predition )\n",
        "    loss = criterion(scores , targets)\n",
        "    # loss which is the mean square error of the scores which is the weights with the\n",
        "    # targets which should be (15) and the yi-y\n",
        "\n",
        "    # backword \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # compute the gradients \n",
        "\n",
        "\n",
        "    # gradient descent or adam step\n",
        "\n",
        "    optimizer.step()\n",
        "    #print(loss.item())\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    #check the accuracy on validation data \n",
        "\n",
        "  def check_accuracy (loader,model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x,y in loader:\n",
        "        x = x.to(device = device )\n",
        "        y = y.to(device = device)\n",
        "        # we are memicing the model and loading the data but this time with a \n",
        "        #trained model \n",
        "        \n",
        "        scores = model(x)\n",
        "        # our (15,1214) tensor \n",
        "        _,prediction = scores.max(1)\n",
        "        # maximum value in the tensor (1) we need the index of it\n",
        "        #print('model prediction = ',prediction , 'y = ',y)\n",
        "        num_correct += (prediction == y).sum()\n",
        "        # compare predition with y our target and get 0 for no and 1 for true or yes \n",
        "        num_samples += prediction.size(0)\n",
        "        # compute the total num of samples \n",
        "\n",
        "      print(f'for{num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "    model.train()\n",
        "\n",
        "  check_accuracy(train_loader,model)\n",
        "  #check_accuracy(Validation_loader , model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv_ZZh3J60WR"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/99_33model.pt')\n",
        "# save the model to the Path which saves the state_dict(from url https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oiLdwchNts_"
      },
      "source": [
        "  def check_accuracy (loader,model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "    your_file = open('newResult.csv', 'ab')\n",
        "    # we create a csv file or open it 'ab' for append binery \n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x,y in loader:\n",
        "        x = x.to(device = device )\n",
        "        y = y.to(device = device)\n",
        "        \n",
        "        scores = model(x)\n",
        "        _,prediction = scores.max(1)\n",
        "        #print('model prediction = ',prediction , 'y = ',y)\n",
        "        num_correct += (prediction == y).sum()\n",
        "        np.savetxt(your_file,prediction.cpu())\n",
        "        num_samples += prediction.size(0)\n",
        "\n",
        "      print(f'for{num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "    your_file.close()\n",
        "    model.train()\n",
        "\n",
        "  check_accuracy(test_loader,model)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEahAqcqrGpJ"
      },
      "source": [
        "def write_csv_results (loader,model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "    your_file = open('resultsWithforeign_75.csv', 'ab')\n",
        "    # we create a csv file or open it 'ab' for append binery \n",
        "    sm = torch.nn.Softmax()\n",
        "    # we use a softmax to compute the probability of each guess of the network \n",
        "    \n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "      for x,_ in loader:\n",
        "        x = x.to(device = device )\n",
        "       \n",
        "        \n",
        "        scores = model(x)\n",
        "        _,prediction = scores.max(1)\n",
        "        #print(scores.max())\n",
        "        #print('model prediction = ',prediction , 'y = ',y)\n",
        "        probabilities = sm(scores) \n",
        "        #print(probabilities.max())\n",
        "        #print(prediction)\n",
        "        if probabilities.max() < 0.75 :\n",
        "          prediction = torch.tensor([-1])\n",
        "          np.savetxt(your_file,prediction.cpu())\n",
        "          # we save the prediction in the open file but first we have to \n",
        "          # send it back to being a cpu \n",
        "        else:\n",
        "          np.savetxt(your_file,prediction.cpu().numpy())  \n",
        "        \n",
        "        \n",
        "\n",
        "      model.train()\n",
        "      your_file.close()\n",
        "\n",
        "write_csv_results(test_loader,model)\n",
        "# using our def \n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}