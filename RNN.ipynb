{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "measured-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets ,models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "mexican-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomDNADataset(Dataset):\n",
    "    def __init__(self, feature_file,label_file, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(label_file)\n",
    "        self.img_features = pd.read_csv(feature_file)\n",
    "        # pandas read the data \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        # no transform is used here \n",
    "        #self.dic= {'-':0,'A' :2**2 , 'C':2**4,'T':2**6,'G':2**7,'N':0}\n",
    "        self.dic= {'-':[0,0,0,0],'A' :[1,0,0,0] , 'C':[0,1,0,0],'T':[0,0,1,0],'G':[0,0,0,1]}\n",
    "        # I learned the sequence of dana from file:///D:/downloads/dna-and-animal-classification.pdf\n",
    "        #and from http://ircamera.as.arizona.edu/Astr2016/text/nucleicacid1.htm \n",
    "        # which showed that there are 4 dominante letters to determine the sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "        # this is just the number rows or samples in the input data as used by \n",
    "        # Dataset class \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    " ############# DNA transform ######################     \n",
    "        DNA = self.img_features.iloc[idx,1]\n",
    "        # so DNA is getting the img_features panda csv read and we are taking the second\n",
    "        #colmn , the first is just ids . idx is generated by pytorch randomly depending on\n",
    "        # if we shuffle the data or not and the number of batch size (part of Dataset class which\n",
    "        # we inheret from )\n",
    "\n",
    "\n",
    "        n = np.array([])\n",
    "        Our_pad = (36*36)-len(DNA)\n",
    "        # here I am using 1296 which is 36*36 , I choose this number based on \n",
    "        # the maximum individual row I found which was around 1058 ,this is used to \n",
    "        # generate a padding which is some how consistent along all .I also chose \n",
    "        # 36 so that when we do conv and maxpool we get a nice number which is \n",
    "        # dividable by 2 How to fix RuntimeError \"Expected object \n",
    "        #of scalar type Float but got scalar type Double for argument\"\n",
    "        # here to get the right dim we muliply by 2 the in dim \n",
    "        \n",
    "        for i in range(len(DNA)):# DNA is our row where we itterate \n",
    "\n",
    "          if DNA[i] in self.dic:\n",
    "            mut = self.dic[DNA[i]]\n",
    "            mut = [x * 2 for x in mut]\n",
    "\n",
    "            n = np.append(n,mut)\n",
    "          else:\n",
    "            n= np.append(n,[0,0,0,0])\n",
    "          \n",
    "          \n",
    "        for pad in range(Our_pad):# we pad the end of the sequence with zeros \n",
    "          n =np.append(n,[0,0,0,0])\n",
    "\n",
    "        \n",
    "        l = np.array(n , dtype=np.float32)\n",
    "        # this was very trick where the base type of any np array is float 64 or \n",
    "        # double but the base type for torch is float 32 so if we convert \n",
    "        # a np array to torch we need to first change it to float32 or we get an error \n",
    "        # which says \n",
    "        Data_array = torch.from_numpy(l)\n",
    "        #transfer a np to torch \n",
    "        #DNA_dense = Data_array\n",
    "        # depending on the type of network we can keep it flat or \n",
    "        # change the dim (resh\n",
    "        DNA_image = Data_array.reshape(1,72,72)\n",
    "\n",
    "        # reshape the dim to be in the form of an image of pix 1 which is \n",
    "        # usally the RGB or gray channel , in our case it is gray ,\n",
    "        # then 36 by 36 which what our padding is doing \n",
    "        #for a 3 chennel image insted of 1 \n",
    "\n",
    "\n",
    "#################################################\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        # we get the labels from a different file \n",
    "\n",
    "        return DNA_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "continuous-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_path = 'C:\\\\Users\\\\yousi\\\\Untitled Folder\\\\dna-barcode-classification/train_features.csv'\n",
    "test_features_path = 'C:\\\\Users\\\\yousi\\\\Untitled Folder\\\\dna-barcode-classification/test_features.csv'\n",
    "train_labels_path = 'C:\\\\Users\\\\yousi\\\\Untitled Folder\\\\dna-barcode-classification/train_labels.csv'\n",
    "test_labels_path = 'C:\\\\Users\\\\yousi\\\\Untitled Folder\\\\dna-barcode-classification/train_labels - Copy.csv'\n",
    "\n",
    "\n",
    "full_dataset= CustomDNADataset(feature_file=train_features_path,label_file=train_labels_path)\n",
    "# so pass the whole set by specifing the path of file to be inputed to our custum class \n",
    "\n",
    "Test_dataset = CustomDNADataset(feature_file=test_features_path,label_file=test_labels_path)\n",
    "# for the test_dataset I am using fake label data which is not accually there just so that I can use \n",
    "# my custom class , it has the same len as the train data ( I choped some in csv file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "informational-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(full_dataset))\n",
    "# this is a creative way to create a validation set by spliting the dataset \n",
    "validation_size = len(full_dataset) - train_size\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(full_dataset, [train_size, validation_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "auburn-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "\n",
    "\n",
    "# hyper parameters \n",
    "input_size = 72\n",
    "sequence_length = 72\n",
    "num_layers = 2\n",
    "hidden_size  = 256\n",
    "\n",
    "num_classes = 1214\n",
    "learning_rate = 0.001\n",
    "batch_size = 15\n",
    "# the batch size appered to be a significant factor on the effectivness of the training\n",
    "# 15 is found to be a good num \n",
    "num_epochs = 3\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "electrical-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size , hidden_size , num_layers , num_classes):\n",
    "        super(RNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size , hidden_size , num_layers,batch_first= True)\n",
    "        self.fc = nn.Linear(hidden_size*sequence_length ,num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.shape) batchSize,72,72\n",
    "        h0 = torch.zeros(self.num_layers , x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers , x.size(0) , self.hidden_size).to(device)\n",
    "        #print(h0.shape) 2,15,275\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #Forward Prop \n",
    "        \n",
    "        out,_ =  self.lstm(x,(h0,c0))\n",
    "        #print(out.shape)15 , 72,256\n",
    "        out = out.reshape(out.shape[0] ,-1)\n",
    "        #print(out.shape) 15,18432\n",
    "        out = self.fc(out)\n",
    "        #print(out.shape) 15 ,1214\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "nutritional-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "train_loader = DataLoader(full_dataset , batch_size=batch_size , \n",
    "                          shuffle = True)\n",
    "Validation_loader = DataLoader(validation_dataset , batch_size=batch_size , \n",
    "                          shuffle = False)\n",
    "test_loader = DataLoader(Test_dataset  , batch_size=1 , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "elementary-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size,hidden_size,num_layers,num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "improved-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and optimizer \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "blond-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for12478/12906 with accuracy 96.68\n",
      "for12687/12906 with accuracy 98.30\n",
      "for12763/12906 with accuracy 98.89\n"
     ]
    }
   ],
   "source": [
    "# train Network\n",
    "for epoch in range (num_epochs):\n",
    "  for batch_ind ,(data, targets) in enumerate(train_loader):\n",
    "    data = data.to(device = device).squeeze(1)\n",
    "    targets = targets.to(device = device)\n",
    "    # we are coping our data from the cpu to the GPU \n",
    "\n",
    "    #print(data.shape)\n",
    "\n",
    "    # forward\n",
    "    scores = model(data)\n",
    "    # this were we send our data to with shape ([batchsize , chennel size , 36,36]in to\n",
    "    # 15,1214 out as our predition )\n",
    "    loss = criterion(scores , targets)\n",
    "    # loss which is the mean square error of the scores which is the weights with the\n",
    "    # targets which should be (15) and the yi-y\n",
    "\n",
    "    # backword \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # compute the gradients \n",
    "\n",
    "\n",
    "    # gradient descent or adam step\n",
    "\n",
    "    optimizer.step()\n",
    "    #print(loss.item())\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #check the accuracy on validation data \n",
    "\n",
    "  def check_accuracy (loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for x,y in loader:\n",
    "        x = x.to(device = device ).squeeze(1)\n",
    "        y = y.to(device = device)\n",
    "        # we are memicing the model and loading the data but this time with a \n",
    "        #trained model \n",
    "        \n",
    "        scores = model(x)\n",
    "        # our (15,1214) tensor \n",
    "        _,prediction = scores.max(1)\n",
    "        # maximum value in the tensor (1) we need the index of it\n",
    "        #print('model prediction = ',prediction , 'y = ',y)\n",
    "        num_correct += (prediction == y).sum()\n",
    "        # compare predition with y our target and get 0 for no and 1 for true or yes \n",
    "        num_samples += prediction.size(0)\n",
    "        # compute the total num of samples \n",
    "\n",
    "      print(f'for{num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "    model.train()\n",
    "\n",
    "  check_accuracy(train_loader,model)\n",
    "  #check_accuracy(Validation_loader , model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "overhead-connecticut",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yousi\\.conda\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(Test_dataset  , batch_size=1 , shuffle=False)\n",
    "def write_csv_results (loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    your_file = open('resultsWithforeign_RNN_LSTM.csv', 'ab')\n",
    "    # we create a csv file or open it 'ab' for append binery \n",
    "    sm = torch.nn.Softmax()\n",
    "    # we use a softmax to compute the probability of each guess of the network \n",
    "    \n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "      for x,_ in loader:\n",
    "        x = x.to(device = device ).squeeze(1)\n",
    "       \n",
    "        \n",
    "        scores = model(x)\n",
    "        _,prediction = scores.max(1)\n",
    "        #print(scores.max())\n",
    "        #print('model prediction = ',prediction , 'y = ',y)\n",
    "        probabilities = sm(scores) \n",
    "        #print(probabilities.max())\n",
    "        #print(prediction)\n",
    "        if probabilities.max() < 0.25 :\n",
    "          prediction = torch.tensor([-1])\n",
    "          np.savetxt(your_file,prediction.cpu())\n",
    "          # we save the prediction in the open file but first we have to \n",
    "          # send it back to being a cpu \n",
    "        else:\n",
    "          np.savetxt(your_file,prediction.cpu().numpy())  \n",
    "        \n",
    "        \n",
    "\n",
    "      model.train()\n",
    "      your_file.close()\n",
    "\n",
    "write_csv_results(test_loader,model)\n",
    "# using our def \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-consultancy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
